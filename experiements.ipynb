{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e0c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models.doc2vec import Doc2Vec,\\\n",
    "    TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d25da",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55438b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_combi_df = pd.read_csv(\n",
    "    \"cleaned_data/Reddit_Combi_cleaned.csv\",   \n",
    "    )\n",
    "\n",
    "reddit_title_df = pd.read_csv(    \n",
    "    \"cleaned_data/Reddit_Title_cleaned.csv\",    \n",
    ")\n",
    "\n",
    "twitter_full_df = pd.read_csv(\n",
    "    \"cleaned_data/Twitter_Full_cleaned.csv\",  \n",
    "    )\n",
    "\n",
    "twitter_non_advert = pd.read_csv(\n",
    "    \"cleaned_data/Twitter_Non-Advert_cleaned.csv\",\n",
    "    )\n",
    "\n",
    "df_files = [reddit_combi_df, reddit_title_df, twitter_full_df, twitter_non_advert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37dd7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'body', 'Body_Title', 'label'], dtype='object')\n",
      "Index(['title', 'label'], dtype='object')\n",
      "Index(['text', 'hashtags', 'label'], dtype='object')\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55643e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'body', 'body_title', 'label'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column name for consistency\n",
    "df_files[0] = df_files[0].rename(columns={\"Body_Title\": \"body_title\"})\n",
    "df_files[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f6646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge columns in dataframed containing text\n",
    "df_files[0][\"full_text\"] = (\n",
    "    df_files[0][\"title\"].fillna(\"\") + \" \" +\n",
    "    df_files[0][\"body\"].fillna(\"\") + \" \" +\n",
    "    df_files[0][\"body_title\"].fillna(\"\") + \" \" \n",
    ")\n",
    "\n",
    "df_files[1][\"full_text\"] = (\n",
    "    df_files[1][\"title\"].fillna(\"\") + \" \"\n",
    ")\n",
    "\n",
    "df_files[2][\"full_text\"] = (\n",
    "    df_files[2][\"text\"].fillna(\"\") + \" \" +\n",
    "    df_files[2][\"hashtags\"].astype(str).fillna(\"\")\n",
    ")\n",
    "\n",
    "df_files[3][\"full_text\"] = (\n",
    "    df_files[3][\"text\"].fillna(\"\") + \" \" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean hashtag symbols from full text, hashtags should only be in Twitter full, but clean from all to maintain consistency\n",
    "def clean_text(text):\n",
    "    text = str(text) \n",
    "    text = re.sub(r\"[\\[\\]']\", \"\", text)  \n",
    "    text = text.replace(\"#\", \"\") \n",
    "    return text\n",
    "\n",
    "for df in df_files:\n",
    "    df[\"full_text\"] = df[\"full_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79510f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'body', 'body_title', 'label', 'full_text'], dtype='object')\n",
      "Index(['title', 'label', 'full_text'], dtype='object')\n",
      "Index(['text', 'hashtags', 'label', 'full_text'], dtype='object')\n",
      "Index(['text', 'label', 'full_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a723b7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>body_title</th>\n",
       "      <th>label</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>envy to other is swallowing me</td>\n",
       "      <td>im from developingcountry, indonesia , and for...</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "      <td>1</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "      <td>um hello .well many can relate im sure. after ...</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "      <td>1</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "      <td>i’ve been diagnosed severe bi polar where you ...</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "      <td>1</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i’m happy again</td>\n",
       "      <td>after my closest friend left me in april, i ha...</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "      <td>0</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "      <td>i am only 15, and yet i feel my life is alread...</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "      <td>1</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     envy to other is swallowing me   \n",
       "1  nothin outta the ordinary. paradise. job stres...   \n",
       "2  almost 49 and the chasm of emptiness has never...   \n",
       "3                                    i’m happy again   \n",
       "4  is it possible to recover from such a traumati...   \n",
       "\n",
       "                                                body  \\\n",
       "0  im from developingcountry, indonesia , and for...   \n",
       "1  um hello .well many can relate im sure. after ...   \n",
       "2  i’ve been diagnosed severe bi polar where you ...   \n",
       "3  after my closest friend left me in april, i ha...   \n",
       "4  i am only 15, and yet i feel my life is alread...   \n",
       "\n",
       "                                          body_title  label  \\\n",
       "0  envy to other is swallowing me im from develop...      1   \n",
       "1  nothin outta the ordinary. paradise. job stres...      1   \n",
       "2  almost 49 and the chasm of emptiness has never...      1   \n",
       "3  i’m happy again after my closest friend left m...      0   \n",
       "4  is it possible to recover from such a traumati...      1   \n",
       "\n",
       "                                           full_text  \n",
       "0  envy to other is swallowing me im from develop...  \n",
       "1  nothin outta the ordinary. paradise. job stres...  \n",
       "2  almost 49 and the chasm of emptiness has never...  \n",
       "3  i’m happy again after my closest friend left m...  \n",
       "4  is it possible to recover from such a traumati...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check full_text is ok\n",
    "\n",
    "df_files[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3dc44",
   "metadata": {},
   "source": [
    "### Merge dataframes, so they contain only one feature column \"full_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aec0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframed containing only full_text and label for all dataframes\n",
    "\n",
    "reddit1 = df_files[0].copy()\n",
    "reddit2 = df_files[1].copy()\n",
    "twitter1 = df_files[2].copy()\n",
    "twitter2 = df_files[3].copy()\n",
    "\n",
    "reddit1 = reddit1.drop(columns=['title', 'body', 'body_title'])\n",
    "reddit2 = reddit2.drop(columns=['title'])\n",
    "twitter1 = twitter1.drop(columns=['text', 'hashtags'])\n",
    "twitter2 = twitter2.drop(columns=['text'])\n",
    "\n",
    "df_files_new = [reddit1, reddit2, twitter1, twitter2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf376db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3123\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "5480\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "8525\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "1972\n",
      "Index(['label', 'full_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files_new:\n",
    "    print(len(df))\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021a73c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          full_text\n",
       "0      1  envy to other is swallowing me im from develop...\n",
       "1      1  nothin outta the ordinary. paradise. job stres...\n",
       "2      1  almost 49 and the chasm of emptiness has never...\n",
       "3      0  i’m happy again after my closest friend left m...\n",
       "4      1  is it possible to recover from such a traumati..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files_new[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ac7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to summarize accuracies\n",
    "accuracy_df = pd.DataFrame(columns=[\n",
    "    \"dataset_split\",   \n",
    "    \"model\",          \n",
    "    \"representation\",  \n",
    "    \"accuracy\"         \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ea36d",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayesian\n",
    "\n",
    "- Is selected as a model, because it's widely used in text classification tasks (https://towardsdatascience.com/multinomial-naive-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6/)\n",
    "\n",
    "## Training data\n",
    "\n",
    "- Three models are trained and data splits are the following\n",
    "    - training: reddit, test and validation: twitter\n",
    "        - how different datasets generalize\n",
    "    - training: twitter, test and validation: reddit\n",
    "        - how different datasets generalize\n",
    "    - training: 80% of all datasets, test and validation: 20% of all datasets\n",
    "        - typical ML data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acd644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data as above\n",
    "\n",
    "X_train1 = pd.concat([reddit1, reddit2])[\"full_text\"]\n",
    "y_train1 = pd.concat([reddit1, reddit2])[\"label\"]\n",
    "X_test1 = pd.concat([twitter1, twitter2])[\"full_text\"]\n",
    "y_test1 = pd.concat([twitter1, twitter2])[\"label\"]\n",
    "\n",
    "X_train2 = pd.concat([twitter1, twitter2])[\"full_text\"]\n",
    "y_train2 = pd.concat([twitter1, twitter2])[\"label\"]\n",
    "X_test2 = pd.concat([reddit1, reddit2])[\"full_text\"]\n",
    "y_test2 = pd.concat([reddit1, reddit2])[\"label\"]\n",
    "\n",
    "X_3 = pd.concat([reddit1, reddit2, twitter1, twitter2])[\"full_text\"]\n",
    "y_3 = pd.concat([reddit1, reddit2, twitter1, twitter2])[\"label\"]\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_3, y_3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db510cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8603\n",
      "10497\n",
      "15280\n"
     ]
    }
   ],
   "source": [
    "# check some lengths\n",
    "print(len(X_train1))\n",
    "print(len(X_test1))\n",
    "print(len(X_train3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f79640",
   "metadata": {},
   "source": [
    "## Train the three multinomial NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a3f2a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for embedding + training\n",
    "# TfidfVectorizer collects a document of TF-IDF features, used in text classification: https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py\n",
    "\n",
    "pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "deb6f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6674287891778603\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.34      0.49      4924\n",
      "           1       0.62      0.95      0.75      5573\n",
      "\n",
      "    accuracy                           0.67     10497\n",
      "   macro avg       0.75      0.65      0.62     10497\n",
      "weighted avg       0.74      0.67      0.63     10497\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oonas\\AppData\\Local\\Temp\\ipykernel_10052\\389802766.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracy_df = pd.concat([\n"
     ]
    }
   ],
   "source": [
    "# train: reddit, test: twitter\n",
    "pip.fit(X_train1, y_train1)\n",
    "y_pred1 = pip.predict(X_test1)\n",
    "acc_MNB_1 = accuracy_score(y_test1, y_pred1)\n",
    "print(\"Accuracy:\", acc_MNB_1)\n",
    "print(classification_report(y_test1, y_pred1))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"MultinomialNB\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": round(acc_MNB_1, 2)\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9621c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8000697431128676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.53      0.66      3189\n",
      "           1       0.78      0.96      0.86      5414\n",
      "\n",
      "    accuracy                           0.80      8603\n",
      "   macro avg       0.83      0.75      0.76      8603\n",
      "weighted avg       0.81      0.80      0.79      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: twitter, test: reddit\n",
    "pip.fit(X_train2, y_train2)\n",
    "y_pred2 = pip.predict(X_test2)\n",
    "acc_MNB_2 = accuracy_score(y_test2, y_pred2)\n",
    "print(\"Accuracy:\", acc_MNB_2)\n",
    "print(classification_report(y_test2, y_pred2))\n",
    "\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"MultinomialNB\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": round(acc_MNB_2, 2)\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e09e3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8109947643979057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.62      0.74      1630\n",
      "           1       0.77      0.96      0.85      2190\n",
      "\n",
      "    accuracy                           0.81      3820\n",
      "   macro avg       0.84      0.79      0.79      3820\n",
      "weighted avg       0.83      0.81      0.80      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: 80%, test: 20%\n",
    "pip.fit(X_train3, y_train3)\n",
    "y_pred3 = pip.predict(X_test3)\n",
    "acc_MNB_3 = accuracy_score(y_test3, y_pred3)\n",
    "print(\"Accuracy:\", acc_MNB_3)\n",
    "print(classification_report(y_test3, y_pred3))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"MultinomialNB\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": round(acc_MNB_3, 2)\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "db073e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>model</th>\n",
       "      <th>representation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dataset_split          model representation  accuracy\n",
       "0  train: Reddit / test: Twitter  MultinomialNB         TF-IDF      0.67\n",
       "1  train: Twitter / test: Reddit  MultinomialNB         TF-IDF      0.80\n",
       "2         train/test 80/20 split  MultinomialNB         TF-IDF      0.81"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de925c",
   "metadata": {},
   "source": [
    "- Third dataset split peformed the best, receiving highest accuracy and f1-score (or tie with second model)\n",
    "- Twitter data as a dataset performed significantly better than reddit data, suggesting the quality of twitter data is better\n",
    "    - Twitter data generalizes better to new observations\n",
    "    - Twitter data itself is almost as good in quality than 80% of all datasets, suggesting twitter data could be used as it's own in model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e389d5d",
   "metadata": {},
   "source": [
    "# SVM and Logstic Regression\n",
    "\n",
    "### Compare models and two embeddings: TF-IDF and Doc2Vec\n",
    "- TF-IDF creates matrix of tf-idf features (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- Doc2Vec learns embeddings of words, numerical vectors (https://www.geeksforgeeks.org/nlp/doc2vec-in-nlp/)\n",
    "\n",
    "### Training and testing data\n",
    "- Use same data splits as in NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "012755f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Logistic regression\n",
    "TF_IDF_log_pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0dc60261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + SVC\n",
    "TF_IDF_svm_pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', svm.SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0082459",
   "metadata": {},
   "source": [
    "### Doc2Vec processing \n",
    "- Vectorize words only once, because it takes over one minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "60f2ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "def doc2vec(X_train, X_test):\n",
    "    tagged_train = [TaggedDocument(words=tokenize(doc), tags=[str(i)]) for i, doc in enumerate(X_train)]\n",
    "\n",
    "    # train Doc2Vev\n",
    "    model = Doc2Vec(vector_size=20, min_count=2, epochs=50)\n",
    "    model.build_vocab(tagged_train)\n",
    "    model.train(tagged_train, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # get document vectors\n",
    "    train_vec = [model.dv[str(i)] for i in range(len(tagged_train))]\n",
    "    test_vec = [model.infer_vector(tokenize(doc)) for doc in X_test]\n",
    "\n",
    "    return np.array(train_vec), np.array(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1fc0b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1_d2v, X_test_1_d2v = doc2vec(X_train1.astype(str).tolist(), X_test1.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d9d7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2_d2v, X_test_2_d2v = doc2vec(X_train2.astype(str).tolist(), X_test2.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d72e8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_d2v, X_test_3_d2v = doc2vec(X_train3.astype(str).tolist(), X_test3.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf08b0",
   "metadata": {},
   "source": [
    "### TF-IDF tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f7529374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: reddit, test: twitter\n",
    "TF_IDF_log_pip.fit(X_train1, y_train1)\n",
    "y_pred1_1 = TF_IDF_log_pip.predict(X_test1)\n",
    "\n",
    "TF_IDF_svm_pip.fit(X_train1, y_train1)\n",
    "y_pred1_2 = TF_IDF_svm_pip.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fea21692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7370677336381823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.71      4924\n",
      "           1       0.74      0.78      0.76      5573\n",
      "\n",
      "    accuracy                           0.74     10497\n",
      "   macro avg       0.74      0.73      0.73     10497\n",
      "weighted avg       0.74      0.74      0.74     10497\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.7304944269791369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      4924\n",
      "           1       0.75      0.75      0.75      5573\n",
      "\n",
      "    accuracy                           0.73     10497\n",
      "   macro avg       0.73      0.73      0.73     10497\n",
      "weighted avg       0.73      0.73      0.73     10497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr = accuracy_score(y_test1, y_pred1_1)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr)\n",
    "print(classification_report(y_test1, y_pred1_1))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_lr\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc = accuracy_score(y_test1, y_pred1_2)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc)\n",
    "print(classification_report(y_test1, y_pred1_2))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_svc\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ac8a9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: twitter, test: reddit\n",
    "TF_IDF_log_pip.fit(X_train2, y_train2)\n",
    "y_pred2_1 = TF_IDF_log_pip.predict(X_test2)\n",
    "\n",
    "TF_IDF_svm_pip.fit(X_train2, y_train2)\n",
    "y_pred2_2 = TF_IDF_svm_pip.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4919dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7927467162617692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72      3189\n",
      "           1       0.83      0.84      0.84      5414\n",
      "\n",
      "    accuracy                           0.79      8603\n",
      "   macro avg       0.78      0.77      0.78      8603\n",
      "weighted avg       0.79      0.79      0.79      8603\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.8029757061490178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.64      0.71      3189\n",
      "           1       0.81      0.90      0.85      5414\n",
      "\n",
      "    accuracy                           0.80      8603\n",
      "   macro avg       0.80      0.77      0.78      8603\n",
      "weighted avg       0.80      0.80      0.80      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr2 = accuracy_score(y_test2, y_pred2_1)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr2)\n",
    "print(classification_report(y_test2, y_pred2_1))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_lr2\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc2 = accuracy_score(y_test2, y_pred2_2)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc2)\n",
    "print(classification_report(y_test2, y_pred2_2))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_svc2\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4abde098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: 80&, test: 20%\n",
    "TF_IDF_log_pip.fit(X_train3, y_train3)\n",
    "y_pred3_1 = TF_IDF_log_pip.predict(X_test3)\n",
    "\n",
    "TF_IDF_svm_pip.fit(X_train3, y_train3)\n",
    "y_pred3_2 = TF_IDF_svm_pip.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "86f30ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.8667539267015707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.84      1630\n",
      "           1       0.86      0.92      0.89      2190\n",
      "\n",
      "    accuracy                           0.87      3820\n",
      "   macro avg       0.87      0.86      0.86      3820\n",
      "weighted avg       0.87      0.87      0.87      3820\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.881413612565445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.86      1630\n",
      "           1       0.88      0.92      0.90      2190\n",
      "\n",
      "    accuracy                           0.88      3820\n",
      "   macro avg       0.88      0.87      0.88      3820\n",
      "weighted avg       0.88      0.88      0.88      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr3 = accuracy_score(y_test3, y_pred3_1)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr3)\n",
    "print(classification_report(y_test3, y_pred3_1))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_lr3\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc3 = accuracy_score(y_test3, y_pred3_2)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc3)\n",
    "print(classification_report(y_test3, y_pred3_2))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_svc3\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ebc33b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>model</th>\n",
       "      <th>representation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.737068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.730494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.792747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.866754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.881414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.605030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.595218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.768104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.544461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.747382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.776963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dataset_split                model representation  \\\n",
       "0   train: Reddit / test: Twitter        MultinomialNB         TF-IDF   \n",
       "1   train: Twitter / test: Reddit        MultinomialNB         TF-IDF   \n",
       "2          train/test 80/20 split        MultinomialNB         TF-IDF   \n",
       "3   train: Reddit / test: Twitter  Logistic Regression         TF-IDF   \n",
       "4   train: Reddit / test: Twitter                  SVM         TF-IDF   \n",
       "5   train: Twitter / test: Reddit  Logistic Regression         TF-IDF   \n",
       "6   train: Twitter / test: Reddit                  SVM         TF-IDF   \n",
       "7          train/test 80/20 split  Logistic Regression         TF-IDF   \n",
       "8          train/test 80/20 split                  SVM         TF-IDF   \n",
       "9   train: Reddit / test: Twitter  Logistic Regression        Doc2Vec   \n",
       "10  train: Reddit / test: Twitter                  SVM        Doc2Vec   \n",
       "11  train: Twitter / test: Reddit  Logistic Regression        Doc2Vec   \n",
       "12  train: Twitter / test: Reddit                  SVM        Doc2Vec   \n",
       "13         train/test 80/20 split  Logistic Regression        Doc2Vec   \n",
       "14         train/test 80/20 split                  SVM        Doc2Vec   \n",
       "\n",
       "    accuracy  \n",
       "0   0.670000  \n",
       "1   0.800000  \n",
       "2   0.810000  \n",
       "3   0.737068  \n",
       "4   0.730494  \n",
       "5   0.792747  \n",
       "6   0.802976  \n",
       "7   0.866754  \n",
       "8   0.881414  \n",
       "9   0.605030  \n",
       "10  0.595218  \n",
       "11  0.768104  \n",
       "12  0.544461  \n",
       "13  0.747382  \n",
       "14  0.776963  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd93409",
   "metadata": {},
   "source": [
    "## Doc2Vec Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc6889c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression()\n",
    "SVC_model = svm.SVC()\n",
    "\n",
    "log_reg_model_2 = LogisticRegression()\n",
    "SVC_model_2 = svm.SVC()\n",
    "\n",
    "log_reg_model_3 = LogisticRegression()\n",
    "SVC_model_3 = svm.SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3bf89",
   "metadata": {},
   "source": [
    "### Training data: Reddit, testing data: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e678d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model.fit(X_train_1_d2v, y_train1)\n",
    "y_pred1_lr_d2v = log_reg_model.predict(X_test_1_d2v) \n",
    "\n",
    "SVC_model.fit(X_train_1_d2v, y_train1)\n",
    "y_pred1_svc_d2v = SVC_model.predict(X_test_1_d2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "deea2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.6050300085738782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.63      4924\n",
      "           1       0.67      0.49      0.57      5573\n",
      "\n",
      "    accuracy                           0.61     10497\n",
      "   macro avg       0.62      0.61      0.60     10497\n",
      "weighted avg       0.62      0.61      0.60     10497\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.5952176812422597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.82      0.66      4924\n",
      "           1       0.71      0.40      0.51      5573\n",
      "\n",
      "    accuracy                           0.60     10497\n",
      "   macro avg       0.63      0.61      0.58     10497\n",
      "weighted avg       0.64      0.60      0.58     10497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr_d2v1 = accuracy_score(y_test1, y_pred1_lr_d2v)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr_d2v1)\n",
    "print(classification_report(y_test1, y_pred1_lr_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_lr_d2v1\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc_d2v1 = accuracy_score(y_test1, y_pred1_svc_d2v)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc_d2v1)\n",
    "print(classification_report(y_test1, y_pred1_svc_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_svc_d2v1\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34609555",
   "metadata": {},
   "source": [
    "### Training data: Twitter, testing data: Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "900ea4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_2.fit(X_train_2_d2v, y_train2)\n",
    "y_pred2_lr_d2v = log_reg_model_2.predict(X_test_2_d2v) \n",
    "\n",
    "SVC_model_2.fit(X_train_2_d2v, y_train2)\n",
    "y_pred2_svc_d2v = SVC_model_2.predict(X_test_2_d2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3c92d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7681041497152156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.60      0.66      3189\n",
      "           1       0.79      0.87      0.83      5414\n",
      "\n",
      "    accuracy                           0.77      8603\n",
      "   macro avg       0.76      0.73      0.74      8603\n",
      "weighted avg       0.76      0.77      0.76      8603\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.5444612344530978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.63      0.50      3189\n",
      "           1       0.69      0.50      0.58      5414\n",
      "\n",
      "    accuracy                           0.54      8603\n",
      "   macro avg       0.56      0.56      0.54      8603\n",
      "weighted avg       0.59      0.54      0.55      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr_d2v2 = accuracy_score(y_test2, y_pred2_lr_d2v)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr_d2v2)\n",
    "print(classification_report(y_test2, y_pred2_lr_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_lr_d2v2\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc_d2v2 = accuracy_score(y_test2, y_pred2_svc_d2v)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc_d2v2)\n",
    "print(classification_report(y_test2, y_pred2_svc_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_svc_d2v2\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43da450",
   "metadata": {},
   "source": [
    "### Training data: 80%, testing data: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ef85bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_3.fit(X_train_3_d2v, y_train3)\n",
    "y_pred3_lr_d2v = log_reg_model_3.predict(X_test_3_d2v) \n",
    "\n",
    "SVC_model_3.fit(X_train_3_d2v, y_train3)\n",
    "y_pred3_svc_d2v = SVC_model_3.predict(X_test_3_d2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "de2d4d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7473821989528796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.64      0.68      1630\n",
      "           1       0.76      0.83      0.79      2190\n",
      "\n",
      "    accuracy                           0.75      3820\n",
      "   macro avg       0.74      0.73      0.74      3820\n",
      "weighted avg       0.75      0.75      0.74      3820\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.7769633507853403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.68      0.72      1630\n",
      "           1       0.78      0.85      0.81      2190\n",
      "\n",
      "    accuracy                           0.78      3820\n",
      "   macro avg       0.78      0.76      0.77      3820\n",
      "weighted avg       0.78      0.78      0.77      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr_d2v3 = accuracy_score(y_test3, y_pred3_lr_d2v)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr_d2v3)\n",
    "print(classification_report(y_test3, y_pred3_lr_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_lr_d2v3\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc_d2v3 = accuracy_score(y_test3, y_pred3_svc_d2v)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc_d2v3)\n",
    "print(classification_report(y_test3, y_pred3_svc_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_svc_d2v3\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "644c3d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>model</th>\n",
       "      <th>representation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.737068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.730494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.792747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.866754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.881414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.605030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.595218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.768104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.544461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.747382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.776963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dataset_split                model representation  \\\n",
       "0   train: Reddit / test: Twitter        MultinomialNB         TF-IDF   \n",
       "1   train: Twitter / test: Reddit        MultinomialNB         TF-IDF   \n",
       "2          train/test 80/20 split        MultinomialNB         TF-IDF   \n",
       "3   train: Reddit / test: Twitter  Logistic Regression         TF-IDF   \n",
       "4   train: Reddit / test: Twitter                  SVM         TF-IDF   \n",
       "5   train: Twitter / test: Reddit  Logistic Regression         TF-IDF   \n",
       "6   train: Twitter / test: Reddit                  SVM         TF-IDF   \n",
       "7          train/test 80/20 split  Logistic Regression         TF-IDF   \n",
       "8          train/test 80/20 split                  SVM         TF-IDF   \n",
       "9   train: Reddit / test: Twitter  Logistic Regression        Doc2Vec   \n",
       "10  train: Reddit / test: Twitter                  SVM        Doc2Vec   \n",
       "11  train: Twitter / test: Reddit  Logistic Regression        Doc2Vec   \n",
       "12  train: Twitter / test: Reddit                  SVM        Doc2Vec   \n",
       "13         train/test 80/20 split  Logistic Regression        Doc2Vec   \n",
       "14         train/test 80/20 split                  SVM        Doc2Vec   \n",
       "\n",
       "    accuracy  \n",
       "0   0.670000  \n",
       "1   0.800000  \n",
       "2   0.810000  \n",
       "3   0.737068  \n",
       "4   0.730494  \n",
       "5   0.792747  \n",
       "6   0.802976  \n",
       "7   0.866754  \n",
       "8   0.881414  \n",
       "9   0.605030  \n",
       "10  0.595218  \n",
       "11  0.768104  \n",
       "12  0.544461  \n",
       "13  0.747382  \n",
       "14  0.776963  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96302a",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "- https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d2b77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fe52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4377d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2deed9",
   "metadata": {},
   "source": [
    "- set max_length = 128 to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f6ddc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(dataframe):\n",
    "    dataframe = dataframe.rename(columns={'full_text': 'text'})\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "553d398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d103a8910b444f4b360f9dbd27c5518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply data formatting to train and test sets\n",
    "datasets = []\n",
    "for df in df_files_new:\n",
    "    datasets.append(format_data(df))\n",
    "    \n",
    "# combine datasets and split into train and tests sets as before\n",
    "full_dataset = concatenate_datasets(datasets)\n",
    "full_dataset = full_dataset.shuffle(seed=42)\n",
    "full_dataset = full_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "dataset = full_dataset.train_test_split(test_size=0.2)\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8deef09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'academicparity much has been written about how to overhaul the system and find a better way to define success in research. but on the ground, the truth is that the system is making young people ill and they need our help academicmentalhealth mentalhealthmatters scipol ',\n",
       " 'input_ids': [101,\n",
       "  3834,\n",
       "  19362,\n",
       "  3012,\n",
       "  2172,\n",
       "  2038,\n",
       "  2042,\n",
       "  2517,\n",
       "  2055,\n",
       "  2129,\n",
       "  2000,\n",
       "  18181,\n",
       "  1996,\n",
       "  2291,\n",
       "  1998,\n",
       "  2424,\n",
       "  1037,\n",
       "  2488,\n",
       "  2126,\n",
       "  2000,\n",
       "  9375,\n",
       "  3112,\n",
       "  1999,\n",
       "  2470,\n",
       "  1012,\n",
       "  2021,\n",
       "  2006,\n",
       "  1996,\n",
       "  2598,\n",
       "  1010,\n",
       "  1996,\n",
       "  3606,\n",
       "  2003,\n",
       "  2008,\n",
       "  1996,\n",
       "  2291,\n",
       "  2003,\n",
       "  2437,\n",
       "  2402,\n",
       "  2111,\n",
       "  5665,\n",
       "  1998,\n",
       "  2027,\n",
       "  2342,\n",
       "  2256,\n",
       "  2393,\n",
       "  3834,\n",
       "  26901,\n",
       "  20192,\n",
       "  24658,\n",
       "  5177,\n",
       "  20192,\n",
       "  24658,\n",
       "  18900,\n",
       "  7747,\n",
       "  16596,\n",
       "  18155,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ceck format\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c2fc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03518ed",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f232aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dc6a1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1261454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "329f95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5049113d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3820' max='3820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3820/3820 1:00:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.451800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.472200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.417800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.353400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3820, training_loss=0.36184345827052733, metrics={'train_runtime': 3625.0038, 'train_samples_per_second': 4.215, 'train_steps_per_second': 1.054, 'total_flos': 384328284661584.0, 'train_loss': 0.36184345827052733, 'epoch': 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Transformer\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.0,\n",
    "    #evaluation_strategy=\"no\",           \n",
    "    save_strategy=\"no\",               \n",
    "    logging_steps=50,                        \n",
    "    report_to=\"none\",                     \n",
    "    push_to_hub=False,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59abf517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='955' max='955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [955/955 03:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9193717277486911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oonas\\AppData\\Local\\Temp\\ipykernel_10964\\3645557917.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracy_df = pd.concat([\n"
     ]
    }
   ],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "accuracy = eval_result['eval_accuracy']\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"Transformer\",\n",
    "        \"representation\": \"Tokenizer\",\n",
    "        \"accuracy\": accuracy\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7ff01",
   "metadata": {},
   "source": [
    "### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92fc91b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Transformer\\\\tokenizer_config.json',\n",
       " 'Transformer\\\\special_tokens_map.json',\n",
       " 'Transformer\\\\vocab.txt',\n",
       " 'Transformer\\\\added_tokens.json',\n",
       " 'Transformer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"Transformer\")\n",
    "tokenizer.save_pretrained(\"Transformer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9b51c3",
   "metadata": {},
   "source": [
    "### Take model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d683836",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"Transformer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Transformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
