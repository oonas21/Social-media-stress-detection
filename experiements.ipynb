{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21e0c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models.doc2vec import Doc2Vec,\\\n",
    "    TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d25da",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55438b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_combi_df = pd.read_csv(\n",
    "    \"Reddit_Combi_cleaned.csv\",   \n",
    "    )\n",
    "\n",
    "reddit_title_df = pd.read_csv(    \n",
    "    \"Reddit_Title_cleaned.csv\",    \n",
    ")\n",
    "\n",
    "twitter_full_df = pd.read_csv(\n",
    "    \"Twitter_Full_cleaned.csv\",  \n",
    "    )\n",
    "\n",
    "twitter_non_advert = pd.read_csv(\n",
    "    \"Twitter_Non-Advert_cleaned.csv\",\n",
    "    )\n",
    "\n",
    "df_files = [reddit_combi_df, reddit_title_df, twitter_full_df, twitter_non_advert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37dd7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'body', 'Body_Title', 'label'], dtype='object')\n",
      "Index(['title', 'label'], dtype='object')\n",
      "Index(['text', 'hashtags', 'label'], dtype='object')\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55643e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'body', 'body_title', 'label'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column name for consistency\n",
    "df_files[0] = df_files[0].rename(columns={\"Body_Title\": \"body_title\"})\n",
    "df_files[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f6646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge columns in dataframed containing text\n",
    "df_files[0][\"full_text\"] = (\n",
    "    df_files[0][\"title\"].fillna(\"\") + \" \" +\n",
    "    df_files[0][\"body\"].fillna(\"\") + \" \" +\n",
    "    df_files[0][\"body_title\"].fillna(\"\") + \" \" \n",
    ")\n",
    "\n",
    "df_files[1][\"full_text\"] = (\n",
    "    df_files[1][\"title\"].fillna(\"\") + \" \"\n",
    ")\n",
    "\n",
    "df_files[2][\"full_text\"] = (\n",
    "    df_files[2][\"text\"].fillna(\"\") + \" \" +\n",
    "    df_files[2][\"hashtags\"].astype(str).fillna(\"\")\n",
    ")\n",
    "\n",
    "df_files[3][\"full_text\"] = (\n",
    "    df_files[3][\"text\"].fillna(\"\") + \" \" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79510f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'body', 'body_title', 'label', 'full_text'], dtype='object')\n",
      "Index(['title', 'label', 'full_text'], dtype='object')\n",
      "Index(['text', 'hashtags', 'label', 'full_text'], dtype='object')\n",
      "Index(['text', 'label', 'full_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a723b7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>body_title</th>\n",
       "      <th>label</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>envy to other is swallowing me</td>\n",
       "      <td>im from developingcountry, indonesia , and for...</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "      <td>1</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "      <td>um hello .well many can relate im sure. after ...</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "      <td>1</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "      <td>i’ve been diagnosed severe bi polar where you ...</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "      <td>1</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i’m happy again</td>\n",
       "      <td>after my closest friend left me in april, i ha...</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "      <td>0</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "      <td>i am only 15, and yet i feel my life is alread...</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "      <td>1</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     envy to other is swallowing me   \n",
       "1  nothin outta the ordinary. paradise. job stres...   \n",
       "2  almost 49 and the chasm of emptiness has never...   \n",
       "3                                    i’m happy again   \n",
       "4  is it possible to recover from such a traumati...   \n",
       "\n",
       "                                                body  \\\n",
       "0  im from developingcountry, indonesia , and for...   \n",
       "1  um hello .well many can relate im sure. after ...   \n",
       "2  i’ve been diagnosed severe bi polar where you ...   \n",
       "3  after my closest friend left me in april, i ha...   \n",
       "4  i am only 15, and yet i feel my life is alread...   \n",
       "\n",
       "                                          body_title  label  \\\n",
       "0  envy to other is swallowing me im from develop...      1   \n",
       "1  nothin outta the ordinary. paradise. job stres...      1   \n",
       "2  almost 49 and the chasm of emptiness has never...      1   \n",
       "3  i’m happy again after my closest friend left m...      0   \n",
       "4  is it possible to recover from such a traumati...      1   \n",
       "\n",
       "                                           full_text  \n",
       "0  envy to other is swallowing me im from develop...  \n",
       "1  nothin outta the ordinary. paradise. job stres...  \n",
       "2  almost 49 and the chasm of emptiness has never...  \n",
       "3  i’m happy again after my closest friend left m...  \n",
       "4  is it possible to recover from such a traumati...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check full_text is ok\n",
    "\n",
    "df_files[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3dc44",
   "metadata": {},
   "source": [
    "### Merge dataframes, so they contain only one feature column \"full_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aec0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframed containing only full_text and label for all dataframes\n",
    "\n",
    "reddit1 = df_files[0].copy()\n",
    "reddit2 = df_files[1].copy()\n",
    "twitter1 = df_files[2].copy()\n",
    "twitter2 = df_files[3].copy()\n",
    "\n",
    "reddit1 = reddit1.drop(columns=['title', 'body', 'body_title'])\n",
    "reddit2 = reddit2.drop(columns=['title'])\n",
    "twitter1 = twitter1.drop(columns=['text', 'hashtags'])\n",
    "twitter2 = twitter2.drop(columns=['text'])\n",
    "\n",
    "df_files_new = [reddit1, reddit2, twitter1, twitter2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bf376db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3123\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "5480\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "8525\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "1972\n",
      "Index(['label', 'full_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files_new:\n",
    "    print(len(df))\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ea36d",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayesian\n",
    "\n",
    "- Is selected as a model, because it's widely used in text classification tasks (https://towardsdatascience.com/multinomial-naive-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6/)\n",
    "\n",
    "## Training data\n",
    "\n",
    "- Three models are trained and data splits are the following\n",
    "    - training: reddit, test and validation: twitter\n",
    "        - how different datasets generalize\n",
    "    - training: twitter, test and validation: reddit\n",
    "        - how different datasets generalize\n",
    "    - training: 80% of all datasets, test and validation: 20% of all datasets\n",
    "        - typical ML data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acd644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data as above\n",
    "\n",
    "X_train1 = pd.concat([reddit1, reddit2])[\"full_text\"]\n",
    "y_train1 = pd.concat([reddit1, reddit2])[\"label\"]\n",
    "X_test1 = pd.concat([twitter1, twitter2])[\"full_text\"]\n",
    "y_test1 = pd.concat([twitter1, twitter2])[\"label\"]\n",
    "\n",
    "X_train2 = pd.concat([twitter1, twitter2])[\"full_text\"]\n",
    "y_train2 = pd.concat([twitter1, twitter2])[\"label\"]\n",
    "X_test2 = pd.concat([reddit1, reddit2])[\"full_text\"]\n",
    "y_test2 = pd.concat([reddit1, reddit2])[\"label\"]\n",
    "\n",
    "X_3 = pd.concat([reddit1, reddit2, twitter1, twitter2])[\"full_text\"]\n",
    "y_3 = pd.concat([reddit1, reddit2, twitter1, twitter2])[\"label\"]\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_3, y_3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0db510cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8603\n",
      "10497\n",
      "13370\n"
     ]
    }
   ],
   "source": [
    "# check some lengths\n",
    "print(len(X_train1))\n",
    "print(len(X_test1))\n",
    "print(len(X_train3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f79640",
   "metadata": {},
   "source": [
    "## Train the three multinomial NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for embedding + training\n",
    "# TfidfVectorizer collects a document of TF-IDF features, used in text classification: https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py\n",
    "\n",
    "pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6658092788415738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.34      0.49      4924\n",
      "           1       0.62      0.95      0.75      5573\n",
      "\n",
      "    accuracy                           0.67     10497\n",
      "   macro avg       0.74      0.65      0.62     10497\n",
      "weighted avg       0.74      0.67      0.63     10497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: reddit, test: twitter\n",
    "pip.fit(X_train1, y_train1)\n",
    "y_pred1 = pip.predict(X_test1)\n",
    "print(\"Accuracy:\", accuracy_score(y_test1, y_pred1))\n",
    "print(classification_report(y_test1, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7998372660699756\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.54      0.67      3189\n",
      "           1       0.78      0.95      0.86      5414\n",
      "\n",
      "    accuracy                           0.80      8603\n",
      "   macro avg       0.83      0.75      0.76      8603\n",
      "weighted avg       0.81      0.80      0.79      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: twitter, test: reddit\n",
    "pip.fit(X_train2, y_train2)\n",
    "y_pred2 = pip.predict(X_test2)\n",
    "print(\"Accuracy:\", accuracy_score(y_test2, y_pred2))\n",
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8115183246073299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.62      0.74      1630\n",
      "           1       0.77      0.96      0.85      2190\n",
      "\n",
      "    accuracy                           0.81      3820\n",
      "   macro avg       0.84      0.79      0.79      3820\n",
      "weighted avg       0.83      0.81      0.80      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: 80%, test: 20%\n",
    "pip.fit(X_train3, y_train3)\n",
    "y_pred3 = pip.predict(X_test3)\n",
    "print(\"Accuracy:\", accuracy_score(y_test3, y_pred3))\n",
    "print(classification_report(y_test3, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de925c",
   "metadata": {},
   "source": [
    "- Third dataset split peformed the best, receiving highest accuracy and f1-score (or tie with second model)\n",
    "- Twitter data as a dataset performed significantly better than reddit data, suggesting the quality of twitter data is better\n",
    "    - Twitter data generalizes better to new observations\n",
    "    - Twitter data itself is almost as good in quality than 80% of all datasets, suggesting twitter data could be used as it's own in model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e389d5d",
   "metadata": {},
   "source": [
    "# SVM and Logstic Regression\n",
    "\n",
    "### Compare models and two embeddings: TF-IDF and Doc2Vec\n",
    "- TF-IDF creates matrix of tf-idf features (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- Doc2Vec learns embeddings of words, numerical vectors (https://www.geeksforgeeks.org/nlp/doc2vec-in-nlp/)\n",
    "\n",
    "### Training and testing data\n",
    "- Use same data splits as in NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "012755f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Logistic regression\n",
    "TF_IDF_log_pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dc60261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + SVM\n",
    "TF_IDF_svm_pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', svm.SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0082459",
   "metadata": {},
   "source": [
    "### Doc2Vec processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(X, y, model):\n",
    "    tagged_train = [TaggedDocument(words=word_tokenize(doc.lower()),\n",
    "                                  tages=[str(i)]) for i,\n",
    "                                  doc in enumerate(X)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
