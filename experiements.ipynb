{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9ddf3c",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- Download data\n",
    "- Create full_data columns\n",
    "- Create train and datasets\n",
    "- Train models\n",
    "    - Multinomial Naive Bayesian\n",
    "    - Logistic Regression and SVM\n",
    "    - Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "21e0c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models.doc2vec import Doc2Vec,\\\n",
    "    TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ff076581",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d25da",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "55438b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_combi_df = pd.read_csv(\n",
    "    \"cleaned_data/Reddit_Combi_cleaned.csv\",   \n",
    "    )\n",
    "\n",
    "reddit_title_df = pd.read_csv(    \n",
    "    \"cleaned_data/Reddit_Title_cleaned.csv\",    \n",
    ")\n",
    "\n",
    "twitter_full_df = pd.read_csv(\n",
    "    \"cleaned_data/Twitter_Full_cleaned.csv\",  \n",
    "    )\n",
    "\n",
    "twitter_non_advert = pd.read_csv(\n",
    "    \"cleaned_data/Twitter_Non-Advert_cleaned.csv\",\n",
    "    )\n",
    "\n",
    "df_files = [reddit_combi_df, reddit_title_df, twitter_full_df, twitter_non_advert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "37dd7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'body', 'Body_Title', 'label'], dtype='object')\n",
      "Index(['title', 'label'], dtype='object')\n",
      "Index(['text', 'hashtags', 'label'], dtype='object')\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f55643e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'body', 'body_title', 'label'], dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename column name for consistency\n",
    "df_files[0] = df_files[0].rename(columns={\"Body_Title\": \"body_title\"})\n",
    "df_files[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c8f6646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge columns in dataframed containing text\n",
    "df_files[0][\"full_text\"] = (\n",
    "    df_files[0][\"title\"].fillna(\"\") + \" \" +\n",
    "    df_files[0][\"body\"].fillna(\"\") + \" \" +\n",
    "    df_files[0][\"body_title\"].fillna(\"\") + \" \" \n",
    ")\n",
    "\n",
    "df_files[1][\"full_text\"] = (\n",
    "    df_files[1][\"title\"].fillna(\"\") + \" \"\n",
    ")\n",
    "\n",
    "df_files[2][\"full_text\"] = (\n",
    "    df_files[2][\"text\"].fillna(\"\") + \" \" +\n",
    "    df_files[2][\"hashtags\"].astype(str).fillna(\"\")\n",
    ")\n",
    "\n",
    "df_files[3][\"full_text\"] = (\n",
    "    df_files[3][\"text\"].fillna(\"\") + \" \" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "813f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean hashtag symbols from full text, hashtags should only be in Twitter full, but clean from all to maintain consistency\n",
    "def clean_text(text):\n",
    "    text = str(text) \n",
    "    text = re.sub(r\"[\\[\\]']\", \"\", text)  \n",
    "    text = text.replace(\"#\", \"\") \n",
    "    return text\n",
    "\n",
    "for df in df_files:\n",
    "    df[\"full_text\"] = df[\"full_text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "79510f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'body', 'body_title', 'label', 'full_text'], dtype='object')\n",
      "Index(['title', 'label', 'full_text'], dtype='object')\n",
      "Index(['text', 'hashtags', 'label', 'full_text'], dtype='object')\n",
      "Index(['text', 'label', 'full_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files:\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a723b7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>body_title</th>\n",
       "      <th>label</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>envy to other is swallowing me</td>\n",
       "      <td>im from developingcountry, indonesia , and for...</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "      <td>1</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "      <td>um hello .well many can relate im sure. after ...</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "      <td>1</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "      <td>i’ve been diagnosed severe bi polar where you ...</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "      <td>1</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i’m happy again</td>\n",
       "      <td>after my closest friend left me in april, i ha...</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "      <td>0</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "      <td>i am only 15, and yet i feel my life is alread...</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "      <td>1</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     envy to other is swallowing me   \n",
       "1  nothin outta the ordinary. paradise. job stres...   \n",
       "2  almost 49 and the chasm of emptiness has never...   \n",
       "3                                    i’m happy again   \n",
       "4  is it possible to recover from such a traumati...   \n",
       "\n",
       "                                                body  \\\n",
       "0  im from developingcountry, indonesia , and for...   \n",
       "1  um hello .well many can relate im sure. after ...   \n",
       "2  i’ve been diagnosed severe bi polar where you ...   \n",
       "3  after my closest friend left me in april, i ha...   \n",
       "4  i am only 15, and yet i feel my life is alread...   \n",
       "\n",
       "                                          body_title  label  \\\n",
       "0  envy to other is swallowing me im from develop...      1   \n",
       "1  nothin outta the ordinary. paradise. job stres...      1   \n",
       "2  almost 49 and the chasm of emptiness has never...      1   \n",
       "3  i’m happy again after my closest friend left m...      0   \n",
       "4  is it possible to recover from such a traumati...      1   \n",
       "\n",
       "                                           full_text  \n",
       "0  envy to other is swallowing me im from develop...  \n",
       "1  nothin outta the ordinary. paradise. job stres...  \n",
       "2  almost 49 and the chasm of emptiness has never...  \n",
       "3  i’m happy again after my closest friend left m...  \n",
       "4  is it possible to recover from such a traumati...  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check full_text is ok\n",
    "\n",
    "df_files[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3dc44",
   "metadata": {},
   "source": [
    "### Merge dataframes, so they contain only one feature column \"full_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0aec0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframed containing only full_text and label for all dataframes\n",
    "\n",
    "reddit1 = df_files[0].copy()\n",
    "reddit2 = df_files[1].copy()\n",
    "twitter1 = df_files[2].copy()\n",
    "twitter2 = df_files[3].copy()\n",
    "\n",
    "reddit1 = reddit1.drop(columns=['title', 'body', 'body_title'])\n",
    "reddit2 = reddit2.drop(columns=['title'])\n",
    "twitter1 = twitter1.drop(columns=['text', 'hashtags'])\n",
    "twitter2 = twitter2.drop(columns=['text'])\n",
    "\n",
    "df_files_new = [reddit1, reddit2, twitter1, twitter2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4bf376db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3123\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "5480\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "8525\n",
      "Index(['label', 'full_text'], dtype='object')\n",
      "1972\n",
      "Index(['label', 'full_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for df in df_files_new:\n",
    "    print(len(df))\n",
    "    print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "021a73c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>envy to other is swallowing me im from develop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nothin outta the ordinary. paradise. job stres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>almost 49 and the chasm of emptiness has never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i’m happy again after my closest friend left m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>is it possible to recover from such a traumati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          full_text\n",
       "0      1  envy to other is swallowing me im from develop...\n",
       "1      1  nothin outta the ordinary. paradise. job stres...\n",
       "2      1  almost 49 and the chasm of emptiness has never...\n",
       "3      0  i’m happy again after my closest friend left m...\n",
       "4      1  is it possible to recover from such a traumati..."
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files_new[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "79ac7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to summarize accuracies\n",
    "accuracy_df = pd.DataFrame(columns=[\n",
    "    \"dataset_split\",   \n",
    "    \"model\",          \n",
    "    \"representation\",  \n",
    "    \"accuracy\"         \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ea36d",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "- Three models are trained and data splits are the following\n",
    "    - training: reddit, test and validation: twitter\n",
    "        - how different datasets generalize\n",
    "    - training: twitter, test and validation: reddit\n",
    "        - how different datasets generalize\n",
    "    - training: 80% of all datasets, test and validation: 20% of all datasets\n",
    "        - typical ML data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "acd644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data as above\n",
    "\n",
    "X_train1 = pd.concat([reddit1, reddit2])[\"full_text\"]\n",
    "y_train1 = pd.concat([reddit1, reddit2])[\"label\"]\n",
    "X_test1 = pd.concat([twitter1, twitter2])[\"full_text\"]\n",
    "y_test1 = pd.concat([twitter1, twitter2])[\"label\"]\n",
    "\n",
    "X_train2 = pd.concat([twitter1, twitter2])[\"full_text\"]\n",
    "y_train2 = pd.concat([twitter1, twitter2])[\"label\"]\n",
    "X_test2 = pd.concat([reddit1, reddit2])[\"full_text\"]\n",
    "y_test2 = pd.concat([reddit1, reddit2])[\"label\"]\n",
    "\n",
    "X_3 = pd.concat([reddit1, reddit2, twitter1, twitter2])[\"full_text\"]\n",
    "y_3 = pd.concat([reddit1, reddit2, twitter1, twitter2])[\"label\"]\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_3, y_3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0db510cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8603\n",
      "10497\n",
      "15280\n"
     ]
    }
   ],
   "source": [
    "# check some lengths\n",
    "print(len(X_train1))\n",
    "print(len(X_test1))\n",
    "print(len(X_train3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f79640",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayesian\n",
    "\n",
    "- Is selected as a model, because it's widely used in text classification tasks (https://towardsdatascience.com/multinomial-naive-bayes-for-documents-classification-and-natural-language-processing-nlp-e08cc848ce6/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a3f2a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for embedding + training\n",
    "# TfidfVectorizer collects a document of TF-IDF features, used in text classification: https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py\n",
    "\n",
    "pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6674287891778603\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.34      0.49      4924\n",
      "           1       0.62      0.95      0.75      5573\n",
      "\n",
      "    accuracy                           0.67     10497\n",
      "   macro avg       0.75      0.65      0.62     10497\n",
      "weighted avg       0.74      0.67      0.63     10497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: reddit, test: twitter\n",
    "pip.fit(X_train1, y_train1)\n",
    "y_pred1 = pip.predict(X_test1)\n",
    "acc_MNB_1 = accuracy_score(y_test1, y_pred1)\n",
    "print(\"Accuracy:\", acc_MNB_1)\n",
    "print(classification_report(y_test1, y_pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "780cbf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oonas\\AppData\\Local\\Temp\\ipykernel_10964\\3958619609.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  accuracy_df = pd.concat([\n"
     ]
    }
   ],
   "source": [
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"MultinomialNB\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_MNB_1\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8000697431128676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.53      0.66      3189\n",
      "           1       0.78      0.96      0.86      5414\n",
      "\n",
      "    accuracy                           0.80      8603\n",
      "   macro avg       0.83      0.75      0.76      8603\n",
      "weighted avg       0.81      0.80      0.79      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: twitter, test: reddit\n",
    "pip.fit(X_train2, y_train2)\n",
    "y_pred2 = pip.predict(X_test2)\n",
    "acc_MNB_2 = accuracy_score(y_test2, y_pred2)\n",
    "print(\"Accuracy:\", acc_MNB_2)\n",
    "print(classification_report(y_test2, y_pred2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7a69e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"MultinomialNB\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_MNB_2\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e3af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8109947643979057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.62      0.74      1630\n",
      "           1       0.77      0.96      0.85      2190\n",
      "\n",
      "    accuracy                           0.81      3820\n",
      "   macro avg       0.84      0.79      0.79      3820\n",
      "weighted avg       0.83      0.81      0.80      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train: 80%, test: 20%\n",
    "pip.fit(X_train3, y_train3)\n",
    "y_pred3 = pip.predict(X_test3)\n",
    "acc_MNB_3 = accuracy_score(y_test3, y_pred3)\n",
    "print(\"Accuracy:\", acc_MNB_3)\n",
    "print(classification_report(y_test3, y_pred3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "abb52335",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"MultinomialNB\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_MNB_3\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "db073e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>model</th>\n",
       "      <th>representation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.667429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.800070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.810995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dataset_split          model representation  accuracy\n",
       "0  train: Reddit / test: Twitter  MultinomialNB         TF-IDF  0.667429\n",
       "1  train: Twitter / test: Reddit  MultinomialNB         TF-IDF  0.800070\n",
       "2         train/test 80/20 split  MultinomialNB         TF-IDF  0.810995"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de925c",
   "metadata": {},
   "source": [
    "- Third dataset split peformed the best, receiving highest accuracy and f1-score (or tie with second model)\n",
    "- Twitter data as a dataset performed significantly better than reddit data, suggesting the quality of twitter data is better\n",
    "    - Twitter data generalizes better to new observations\n",
    "    - Twitter data itself is almost as good in quality than 80% of all datasets, suggesting twitter data could be used as it's own in model training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e389d5d",
   "metadata": {},
   "source": [
    "# SVM and Logstic Regression\n",
    "\n",
    "### Compare models and two embeddings: TF-IDF and Doc2Vec\n",
    "- TF-IDF creates matrix of tf-idf features (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "- Doc2Vec learns embeddings of words, numerical vectors (https://www.geeksforgeeks.org/nlp/doc2vec-in-nlp/)\n",
    "\n",
    "### Training and testing data\n",
    "- Use same data splits as in NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "012755f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Logistic regression\n",
    "TF_IDF_log_pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0dc60261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + SVC\n",
    "TF_IDF_svm_pip = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',       # remove short, meaningless words\n",
    "        ngram_range=(1,2)        # to make dict smaller\n",
    "    )),\n",
    "    ('nb', svm.SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0082459",
   "metadata": {},
   "source": [
    "### Doc2Vec processing \n",
    "- Vectorize words only once, because it takes over one minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "60f2ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "def doc2vec(X_train, X_test):\n",
    "    tagged_train = [TaggedDocument(words=tokenize(doc), tags=[str(i)]) for i, doc in enumerate(X_train)]\n",
    "\n",
    "    # train Doc2Vev\n",
    "    model = Doc2Vec(vector_size=20, min_count=2, epochs=50)\n",
    "    model.build_vocab(tagged_train)\n",
    "    model.train(tagged_train, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # get document vectors\n",
    "    train_vec = [model.dv[str(i)] for i in range(len(tagged_train))]\n",
    "    test_vec = [model.infer_vector(tokenize(doc)) for doc in X_test]\n",
    "\n",
    "    return np.array(train_vec), np.array(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1fc0b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1_d2v, X_test_1_d2v = doc2vec(X_train1.astype(str).tolist(), X_test1.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9d9d7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2_d2v, X_test_2_d2v = doc2vec(X_train2.astype(str).tolist(), X_test2.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d72e8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3_d2v, X_test_3_d2v = doc2vec(X_train3.astype(str).tolist(), X_test3.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf08b0",
   "metadata": {},
   "source": [
    "### TF-IDF tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f7529374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: reddit, test: twitter\n",
    "TF_IDF_log_pip.fit(X_train1, y_train1)\n",
    "y_pred1_1 = TF_IDF_log_pip.predict(X_test1)\n",
    "\n",
    "TF_IDF_svm_pip.fit(X_train1, y_train1)\n",
    "y_pred1_2 = TF_IDF_svm_pip.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fea21692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7370677336381823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.69      0.71      4924\n",
      "           1       0.74      0.78      0.76      5573\n",
      "\n",
      "    accuracy                           0.74     10497\n",
      "   macro avg       0.74      0.73      0.73     10497\n",
      "weighted avg       0.74      0.74      0.74     10497\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.7304944269791369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      4924\n",
      "           1       0.75      0.75      0.75      5573\n",
      "\n",
      "    accuracy                           0.73     10497\n",
      "   macro avg       0.73      0.73      0.73     10497\n",
      "weighted avg       0.73      0.73      0.73     10497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr = accuracy_score(y_test1, y_pred1_1)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr)\n",
    "print(classification_report(y_test1, y_pred1_1))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_lr\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc = accuracy_score(y_test1, y_pred1_2)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc)\n",
    "print(classification_report(y_test1, y_pred1_2))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_svc\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ac8a9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: twitter, test: reddit\n",
    "TF_IDF_log_pip.fit(X_train2, y_train2)\n",
    "y_pred2_1 = TF_IDF_log_pip.predict(X_test2)\n",
    "\n",
    "TF_IDF_svm_pip.fit(X_train2, y_train2)\n",
    "y_pred2_2 = TF_IDF_svm_pip.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4919dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7927467162617692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72      3189\n",
      "           1       0.83      0.84      0.84      5414\n",
      "\n",
      "    accuracy                           0.79      8603\n",
      "   macro avg       0.78      0.77      0.78      8603\n",
      "weighted avg       0.79      0.79      0.79      8603\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.8029757061490178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.64      0.71      3189\n",
      "           1       0.81      0.90      0.85      5414\n",
      "\n",
      "    accuracy                           0.80      8603\n",
      "   macro avg       0.80      0.77      0.78      8603\n",
      "weighted avg       0.80      0.80      0.80      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr2 = accuracy_score(y_test2, y_pred2_1)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr2)\n",
    "print(classification_report(y_test2, y_pred2_1))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_lr2\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc2 = accuracy_score(y_test2, y_pred2_2)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc2)\n",
    "print(classification_report(y_test2, y_pred2_2))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_svc2\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4abde098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: 80&, test: 20%\n",
    "TF_IDF_log_pip.fit(X_train3, y_train3)\n",
    "y_pred3_1 = TF_IDF_log_pip.predict(X_test3)\n",
    "\n",
    "TF_IDF_svm_pip.fit(X_train3, y_train3)\n",
    "y_pred3_2 = TF_IDF_svm_pip.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "86f30ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.8667539267015707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.84      1630\n",
      "           1       0.86      0.92      0.89      2190\n",
      "\n",
      "    accuracy                           0.87      3820\n",
      "   macro avg       0.87      0.86      0.86      3820\n",
      "weighted avg       0.87      0.87      0.87      3820\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.881413612565445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.86      1630\n",
      "           1       0.88      0.92      0.90      2190\n",
      "\n",
      "    accuracy                           0.88      3820\n",
      "   macro avg       0.88      0.87      0.88      3820\n",
      "weighted avg       0.88      0.88      0.88      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr3 = accuracy_score(y_test3, y_pred3_1)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr3)\n",
    "print(classification_report(y_test3, y_pred3_1))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_lr3\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc3 = accuracy_score(y_test3, y_pred3_2)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc3)\n",
    "print(classification_report(y_test3, y_pred3_2))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"TF-IDF\",\n",
    "        \"accuracy\": acc_svc3\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ebc33b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>model</th>\n",
       "      <th>representation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.667429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.800070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.810995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.737068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.730494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.792747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.866754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.881414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dataset_split                model representation  accuracy\n",
       "0  train: Reddit / test: Twitter        MultinomialNB         TF-IDF  0.667429\n",
       "1  train: Twitter / test: Reddit        MultinomialNB         TF-IDF  0.800070\n",
       "2         train/test 80/20 split        MultinomialNB         TF-IDF  0.810995\n",
       "3  train: Reddit / test: Twitter  Logistic Regression         TF-IDF  0.737068\n",
       "4  train: Reddit / test: Twitter                  SVM         TF-IDF  0.730494\n",
       "5  train: Twitter / test: Reddit  Logistic Regression         TF-IDF  0.792747\n",
       "6  train: Twitter / test: Reddit                  SVM         TF-IDF  0.802976\n",
       "7         train/test 80/20 split  Logistic Regression         TF-IDF  0.866754\n",
       "8         train/test 80/20 split                  SVM         TF-IDF  0.881414"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd93409",
   "metadata": {},
   "source": [
    "## Doc2Vec Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dc6889c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model = LogisticRegression()\n",
    "SVC_model = svm.SVC()\n",
    "\n",
    "log_reg_model_2 = LogisticRegression()\n",
    "SVC_model_2 = svm.SVC()\n",
    "\n",
    "log_reg_model_3 = LogisticRegression()\n",
    "SVC_model_3 = svm.SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3bf89",
   "metadata": {},
   "source": [
    "### Training data: Reddit, testing data: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e678d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model.fit(X_train_1_d2v, y_train1)\n",
    "y_pred1_lr_d2v = log_reg_model.predict(X_test_1_d2v) \n",
    "\n",
    "SVC_model.fit(X_train_1_d2v, y_train1)\n",
    "y_pred1_svc_d2v = SVC_model.predict(X_test_1_d2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "deea2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.6133180908831095\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.74      0.64      4924\n",
      "           1       0.68      0.50      0.58      5573\n",
      "\n",
      "    accuracy                           0.61     10497\n",
      "   macro avg       0.63      0.62      0.61     10497\n",
      "weighted avg       0.63      0.61      0.61     10497\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.5997904163094218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.81      0.65      4924\n",
      "           1       0.71      0.42      0.53      5573\n",
      "\n",
      "    accuracy                           0.60     10497\n",
      "   macro avg       0.63      0.61      0.59     10497\n",
      "weighted avg       0.63      0.60      0.59     10497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr_d2v1 = accuracy_score(y_test1, y_pred1_lr_d2v)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr_d2v1)\n",
    "print(classification_report(y_test1, y_pred1_lr_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_lr_d2v1\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc_d2v1 = accuracy_score(y_test1, y_pred1_svc_d2v)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc_d2v1)\n",
    "print(classification_report(y_test1, y_pred1_svc_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Reddit / test: Twitter\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_svc_d2v1\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34609555",
   "metadata": {},
   "source": [
    "### Training data: Twitter, testing data: Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "900ea4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_2.fit(X_train_2_d2v, y_train2)\n",
    "y_pred2_lr_d2v = log_reg_model_2.predict(X_test_2_d2v) \n",
    "\n",
    "SVC_model_2.fit(X_train_2_d2v, y_train2)\n",
    "y_pred2_svc_d2v = SVC_model_2.predict(X_test_2_d2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3c92d7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7718237824014879\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.61      0.67      3189\n",
      "           1       0.79      0.87      0.83      5414\n",
      "\n",
      "    accuracy                           0.77      8603\n",
      "   macro avg       0.76      0.74      0.75      8603\n",
      "weighted avg       0.77      0.77      0.77      8603\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.5500406834825061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.64      0.51      3189\n",
      "           1       0.70      0.50      0.58      5414\n",
      "\n",
      "    accuracy                           0.55      8603\n",
      "   macro avg       0.56      0.57      0.55      8603\n",
      "weighted avg       0.60      0.55      0.56      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr_d2v2 = accuracy_score(y_test2, y_pred2_lr_d2v)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr_d2v2)\n",
    "print(classification_report(y_test2, y_pred2_lr_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_lr_d2v2\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc_d2v2 = accuracy_score(y_test2, y_pred2_svc_d2v)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc_d2v2)\n",
    "print(classification_report(y_test2, y_pred2_svc_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train: Twitter / test: Reddit\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_svc_d2v2\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43da450",
   "metadata": {},
   "source": [
    "### Training data: 80%, testing data: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ef85bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_3.fit(X_train_3_d2v, y_train3)\n",
    "y_pred3_lr_d2v = log_reg_model_3.predict(X_test_3_d2v) \n",
    "\n",
    "SVC_model_3.fit(X_train_3_d2v, y_train3)\n",
    "y_pred3_svc_d2v = SVC_model_3.predict(X_test_3_d2v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "de2d4d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.7473821989528796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.65      0.69      1630\n",
      "           1       0.76      0.82      0.79      2190\n",
      "\n",
      "    accuracy                           0.75      3820\n",
      "   macro avg       0.74      0.73      0.74      3820\n",
      "weighted avg       0.75      0.75      0.75      3820\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.7832460732984293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73      1630\n",
      "           1       0.79      0.85      0.82      2190\n",
      "\n",
      "    accuracy                           0.78      3820\n",
      "   macro avg       0.78      0.77      0.78      3820\n",
      "weighted avg       0.78      0.78      0.78      3820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "acc_lr_d2v3 = accuracy_score(y_test3, y_pred3_lr_d2v)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy:\", acc_lr_d2v3)\n",
    "print(classification_report(y_test3, y_pred3_lr_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_lr_d2v3\n",
    "    }])\n",
    "], ignore_index=True)\n",
    "\n",
    "# SVM\n",
    "acc_svc_d2v3 = accuracy_score(y_test3, y_pred3_svc_d2v)\n",
    "print(\"SVM\")\n",
    "print(\"Accuracy:\", acc_svc_d2v3)\n",
    "print(classification_report(y_test3, y_pred3_svc_d2v))\n",
    "\n",
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"SVM\",\n",
    "        \"representation\": \"Doc2Vec\",\n",
    "        \"accuracy\": acc_svc_d2v3\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "644c3d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>model</th>\n",
       "      <th>representation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.667429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.800070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.810995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.737068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.730494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.792747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.866754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.881414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.613318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.599790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.771824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.550041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.747382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.783246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dataset_split                model representation  \\\n",
       "0   train: Reddit / test: Twitter        MultinomialNB         TF-IDF   \n",
       "1   train: Twitter / test: Reddit        MultinomialNB         TF-IDF   \n",
       "2          train/test 80/20 split        MultinomialNB         TF-IDF   \n",
       "3   train: Reddit / test: Twitter  Logistic Regression         TF-IDF   \n",
       "4   train: Reddit / test: Twitter                  SVM         TF-IDF   \n",
       "5   train: Twitter / test: Reddit  Logistic Regression         TF-IDF   \n",
       "6   train: Twitter / test: Reddit                  SVM         TF-IDF   \n",
       "7          train/test 80/20 split  Logistic Regression         TF-IDF   \n",
       "8          train/test 80/20 split                  SVM         TF-IDF   \n",
       "9   train: Reddit / test: Twitter  Logistic Regression        Doc2Vec   \n",
       "10  train: Reddit / test: Twitter                  SVM        Doc2Vec   \n",
       "11  train: Twitter / test: Reddit  Logistic Regression        Doc2Vec   \n",
       "12  train: Twitter / test: Reddit                  SVM        Doc2Vec   \n",
       "13         train/test 80/20 split  Logistic Regression        Doc2Vec   \n",
       "14         train/test 80/20 split                  SVM        Doc2Vec   \n",
       "\n",
       "    accuracy  \n",
       "0   0.667429  \n",
       "1   0.800070  \n",
       "2   0.810995  \n",
       "3   0.737068  \n",
       "4   0.730494  \n",
       "5   0.792747  \n",
       "6   0.802976  \n",
       "7   0.866754  \n",
       "8   0.881414  \n",
       "9   0.613318  \n",
       "10  0.599790  \n",
       "11  0.771824  \n",
       "12  0.550041  \n",
       "13  0.747382  \n",
       "14  0.783246  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96302a",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "- https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0d2b77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c2fe52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b4377d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A4B5853710>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 91b440b7-231f-49a3-a26c-66aee21ed136)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A49212DFD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d017e8d3-e70e-42eb-ba95-fe06ed56d8d9)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A49213AF60>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f695e00d-ae00-46ce-b8c7-ae345b4e2a88)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A492139C40>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3f96bcba-36d3-493d-8157-bd69b6ade544)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A49213BC80>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 39905f91-bc7d-479b-8a2a-f1c5471ce412)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A4921391F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 7f522647-3947-403d-a35e-84b20995552c)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2deed9",
   "metadata": {},
   "source": [
    "- set max_length = 128 to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6f6ddc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(dataframe):\n",
    "    dataframe = dataframe.rename(columns={'full_text': 'text'})\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    return dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "553d398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785f07c5f57644dcb837e9d7ef03acc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply data formatting to train and test sets\n",
    "datasets = []\n",
    "for df in df_files_new:\n",
    "    datasets.append(format_data(df))\n",
    "    \n",
    "# combine datasets and split into train and tests sets as before\n",
    "full_dataset = concatenate_datasets(datasets)\n",
    "full_dataset = full_dataset.shuffle(seed=42)\n",
    "full_dataset = full_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "dataset = full_dataset.train_test_split(test_size=0.2)\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c8deef09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'homework is taking up my life i am a student who was online for the first 4 months of school, and have been in school a week now. i get 6-8 hours of homework a night! i don’t know how to manage it on top of other stuff going on in my life! any advice? homework is taking up my life i am a student who was online for the first 4 months of school, and have been in school a week now. i get 6-8 hours of homework a night! i don’t know how to manage it on top of other stuff going on in my life! any advice? ',\n",
       " 'input_ids': [101,\n",
       "  19453,\n",
       "  2003,\n",
       "  2635,\n",
       "  2039,\n",
       "  2026,\n",
       "  2166,\n",
       "  1045,\n",
       "  2572,\n",
       "  1037,\n",
       "  3076,\n",
       "  2040,\n",
       "  2001,\n",
       "  3784,\n",
       "  2005,\n",
       "  1996,\n",
       "  2034,\n",
       "  1018,\n",
       "  2706,\n",
       "  1997,\n",
       "  2082,\n",
       "  1010,\n",
       "  1998,\n",
       "  2031,\n",
       "  2042,\n",
       "  1999,\n",
       "  2082,\n",
       "  1037,\n",
       "  2733,\n",
       "  2085,\n",
       "  1012,\n",
       "  1045,\n",
       "  2131,\n",
       "  1020,\n",
       "  1011,\n",
       "  1022,\n",
       "  2847,\n",
       "  1997,\n",
       "  19453,\n",
       "  1037,\n",
       "  2305,\n",
       "  999,\n",
       "  1045,\n",
       "  2123,\n",
       "  1521,\n",
       "  1056,\n",
       "  2113,\n",
       "  2129,\n",
       "  2000,\n",
       "  6133,\n",
       "  2009,\n",
       "  2006,\n",
       "  2327,\n",
       "  1997,\n",
       "  2060,\n",
       "  4933,\n",
       "  2183,\n",
       "  2006,\n",
       "  1999,\n",
       "  2026,\n",
       "  2166,\n",
       "  999,\n",
       "  2151,\n",
       "  6040,\n",
       "  1029,\n",
       "  19453,\n",
       "  2003,\n",
       "  2635,\n",
       "  2039,\n",
       "  2026,\n",
       "  2166,\n",
       "  1045,\n",
       "  2572,\n",
       "  1037,\n",
       "  3076,\n",
       "  2040,\n",
       "  2001,\n",
       "  3784,\n",
       "  2005,\n",
       "  1996,\n",
       "  2034,\n",
       "  1018,\n",
       "  2706,\n",
       "  1997,\n",
       "  2082,\n",
       "  1010,\n",
       "  1998,\n",
       "  2031,\n",
       "  2042,\n",
       "  1999,\n",
       "  2082,\n",
       "  1037,\n",
       "  2733,\n",
       "  2085,\n",
       "  1012,\n",
       "  1045,\n",
       "  2131,\n",
       "  1020,\n",
       "  1011,\n",
       "  1022,\n",
       "  2847,\n",
       "  1997,\n",
       "  19453,\n",
       "  1037,\n",
       "  2305,\n",
       "  999,\n",
       "  1045,\n",
       "  2123,\n",
       "  1521,\n",
       "  1056,\n",
       "  2113,\n",
       "  2129,\n",
       "  2000,\n",
       "  6133,\n",
       "  2009,\n",
       "  2006,\n",
       "  2327,\n",
       "  1997,\n",
       "  2060,\n",
       "  4933,\n",
       "  2183,\n",
       "  2006,\n",
       "  1999,\n",
       "  2026,\n",
       "  2166,\n",
       "  999,\n",
       "  2151,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ceck format\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1c2fc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03518ed",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4f232aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\oonas\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--accuracy\\f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Sun Nov 23 09:37:44 2025) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dc6a1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1261454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "329f95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A4B829E4E0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 55af2dcb-8266-4e94-8aa6-6d0fa4b4ef8b)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A4A82FEF90>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 08f6d7e6-3d9a-4286-9b4e-49f8c95dc376)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A4A82FE0F0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 17796eed-9aa2-4423-8917-7e066e3b4aca)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A4A82FEA20>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 20ad27e2-ff85-42c6-a4f8-72846d5ec76b)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A490B19CA0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 3e437f59-3de1-41da-bee9-c8a40f5c93ca)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /distilbert/distilbert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001A4A0B39040>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 4c80d8ab-4d7f-4b93-bda9-005d536c6779)')' thrown while requesting HEAD https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/config.json\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5049113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"Transformer\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.0,\n",
    "        #evaluation_strategy=\"no\",           \n",
    "        save_strategy=\"no\",               \n",
    "        logging_steps=50,                        \n",
    "        report_to=\"none\",                     \n",
    "        push_to_hub=False,\n",
    "        no_cuda=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "70f7b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    trainer.model.save_pretrained(\"Transformer_backup\")\n",
    "    tokenizer.save_pretrained(\"Transformer_backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0428c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"Transformer_backup\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Transformer_backup\")\n",
    "\n",
    "new_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_ds,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f36c676b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='955' max='955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [955/955 03:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9683246073298429\n"
     ]
    }
   ],
   "source": [
    "eval_result = new_trainer.evaluate()\n",
    "print(\"Accuracy:\", eval_result[\"eval_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e6c7eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.concat([\n",
    "    accuracy_df,\n",
    "    pd.DataFrame([{\n",
    "        \"dataset_split\": \"train/test 80/20 split\",\n",
    "        \"model\": \"Transformer\",\n",
    "        \"representation\": \"Tokenizer\",\n",
    "        \"accuracy\": eval_result[\"eval_accuracy\"]\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "68aa984f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_split</th>\n",
       "      <th>model</th>\n",
       "      <th>representation</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.667429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.800070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.810995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.737068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.730494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.792747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.802976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.866754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.881414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.613318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train: Reddit / test: Twitter</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.599790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.771824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train: Twitter / test: Reddit</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.550041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.747382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Doc2Vec</td>\n",
       "      <td>0.783246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train/test 80/20 split</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>Tokenizer</td>\n",
       "      <td>0.968325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dataset_split                model representation  \\\n",
       "0   train: Reddit / test: Twitter        MultinomialNB         TF-IDF   \n",
       "1   train: Twitter / test: Reddit        MultinomialNB         TF-IDF   \n",
       "2          train/test 80/20 split        MultinomialNB         TF-IDF   \n",
       "3   train: Reddit / test: Twitter  Logistic Regression         TF-IDF   \n",
       "4   train: Reddit / test: Twitter                  SVM         TF-IDF   \n",
       "5   train: Twitter / test: Reddit  Logistic Regression         TF-IDF   \n",
       "6   train: Twitter / test: Reddit                  SVM         TF-IDF   \n",
       "7          train/test 80/20 split  Logistic Regression         TF-IDF   \n",
       "8          train/test 80/20 split                  SVM         TF-IDF   \n",
       "9   train: Reddit / test: Twitter  Logistic Regression        Doc2Vec   \n",
       "10  train: Reddit / test: Twitter                  SVM        Doc2Vec   \n",
       "11  train: Twitter / test: Reddit  Logistic Regression        Doc2Vec   \n",
       "12  train: Twitter / test: Reddit                  SVM        Doc2Vec   \n",
       "13         train/test 80/20 split  Logistic Regression        Doc2Vec   \n",
       "14         train/test 80/20 split                  SVM        Doc2Vec   \n",
       "15         train/test 80/20 split          Transformer      Tokenizer   \n",
       "\n",
       "    accuracy  \n",
       "0   0.667429  \n",
       "1   0.800070  \n",
       "2   0.810995  \n",
       "3   0.737068  \n",
       "4   0.730494  \n",
       "5   0.792747  \n",
       "6   0.802976  \n",
       "7   0.866754  \n",
       "8   0.881414  \n",
       "9   0.613318  \n",
       "10  0.599790  \n",
       "11  0.771824  \n",
       "12  0.550041  \n",
       "13  0.747382  \n",
       "14  0.783246  \n",
       "15  0.968325  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a4e6990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df.to_csv('accuracies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
